[["index.html", "Financial Econometrics - Tutorials About", " Financial Econometrics - Tutorials Alessandro Ciancetta Last update: February 09, 2024 About TA materials for the Financial Econometrics course held by Prof. Christian Brownlees at the Barcelona School of Economics. "],["session01.html", "Session 1 Introduction to time series 1.1 Stochastic processes and dependence 1.2 Asymptotic results 1.3 Empirical moments and summary statistics 1.4 Hypothesis testing", " Session 1 Introduction to time series 1.1 Stochastic processes and dependence Stochastic processes are a tool for modeling dependence in consecutive random variables \\(\\{\\dots, Y_{-2}, Y_{-1}, Y_{0}, Y_{1}, Y_{2} \\dots\\}\\). However, in practice, when we observe an empirical time series we are considering one, truncated realization of the stochastic process, \\(\\{y_1, y_2, \\dots, y_T\\}\\). As it is easy to imagine, this can cause some issues in studying the properties of an empirical time series. First, because we can only study the finite dimensional distribution of the process. Second, because our task is to learn something about the process by knowing only one realization of it. To overcome this limitations we need assumptions. In particular, two common assumptions in time series analysis are, loosely speaking: stationarity: the observed values in the sequence come from the same distribution, so that it is possible to learn from the past observations and to generalize the results to the entire, infinite stochastic process ergodicity: values observed far away in time can be considered as independent, and hence if we have enough observations the empirical time series is representative of the entire distribution of the stochastic process Under these (or similar) assumptions, we can use the observations from a single empirical time series to learn the parameters of our models. Stationarity Consider the example in the plot below. If we had only observations laying between two changepoints we would not be able to retrieve the dynamics of the underlying process. For instance, if we observed a series ending before the first changepoint, we would have no information about the future realizations of the process. Indeed, future observations would come from different distributions that we could not learn from available information. ## Example 1: non-stationary process # simulation t_max &lt;- 500 y &lt;- rep(NA, t_max) for (t in 1:t_max) { if (t&lt;=100) { y[t] &lt;- rnorm(1, mean = 0, sd = 0.1) } if (t&gt;100 &amp; t&lt;=250) { y[t] &lt;- rnorm(1, mean = 0.5, sd = 0.1) } if (t&gt;250 &amp; t&lt;=400) { y[t] &lt;- rnorm(1, mean = 0, sd = 0.2) } if (t&gt;400 &amp; t&lt;=t_max) { y[t] &lt;- 0.01*(t-400) + rnorm(1, mean = -0.2, sd = 0.02) } } # plot plot.ts(y, main = &quot;Non-stationary process&quot;, xaxt = &quot;n&quot;) abline(v = c(100, 250, 400), lty = 2) axis(1, at=c(100, 250, 400), labels = c(&quot;Changepoint 1&quot;, &quot;Changepoint 2&quot;, &quot;Changepoint 3&quot;)) Ergodicity Let \\(\\{z_t\\}_{t = 1}^{T} \\stackrel{iid}{\\sim} \\mathcal{N}(0, 1)\\). Consider the two following data-generating processes: \\[ \\begin{aligned} y_t &amp;= U_0 + 0.25z_t, \\quad \\text{with } U_0 \\sim \\mathcal{N}(0, 100) \\\\[1ex] x_t &amp;= z_t + z_{t-1} \\end{aligned} \\] We have: \\[ \\begin{aligned} \\mathbb{E}[y_t] &amp;= \\mathbb{E}[x_t] = 0 \\\\[1em] \\text{cov}(y_t, y_{t-k}) &amp;= \\begin{cases} 100+0.25^2 &amp; k = 0 \\\\ 100 &amp; k \\neq 0 \\end{cases} \\\\[1ex] \\text{cov}(x_t, x_{t-k}) &amp;= \\begin{cases} 2 &amp; k = 0 \\\\ 1 &amp; k = 1 \\\\ 0 &amp; k \\geq 2 \\end{cases} \\end{aligned} \\] We now simulate the two data generating processes in R. We want to draw many sequences \\(\\{y_1, \\dots, y_T\\}^{(s)}, \\{x_1, \\dots, x_T\\}^{(s)}\\) for \\(s = 1, \\dots, S\\) simulations to assess whether observing a single time series is enough to estimate the mean of the processes. ## Example 2: weak dependence ## Simulate process y_t # initialize TxS matrix to store result # (each column is a simulated time series {y_1, ..., y_T}^(s) ) nsim &lt;- 3 y_list &lt;- matrix(rep(NA, nsim*t_max), nrow = t_max, ncol = nsim) # simulation for (sim in 1:nsim) { set.seed(sim+123) U0 &lt;- rnorm(1, sd = 10) z &lt;- rnorm(t_max) y_list[,sim] &lt;- U0 + 0.25*z } As the plot below shows, if we observe a single simulated time series we cannot correctly estimate the mean of the process \\(\\mathbb{E}[y_t]\\) using the sample mean \\(\\bar{y}_T\\). What the sample mean is actually estimating in this case is the conditional expctation of \\(y_t\\) given the specific draw of the random intercept \\(U_0\\) in simulation \\(s\\): \\(\\mathbb{E}\\left[y_t | U_0 = U_0^{(s)}\\right] = U_0^{(s)}\\). Notice that in this case, if we observe a new point for stochastic process \\(y_t\\) (green points in the plot) we are likely not able to forecast it correctly based on the previous observations of a single time series. # plot plot.ts(y_list[,1], ylim = c(-25, 25), main = &quot;Realizations of non-ergodic series&quot;, ylab = &quot;y&quot;) lines(y_list[,2], col = &quot;steelblue&quot;) lines(y_list[,3], col = &quot;tomato&quot;) for (point in 1:100){ points(x = 501, rnorm(1, 0, 10)+rnorm(1, 0, 1), col = &quot;darkgreen&quot;, pch = 4) } abline(h = 0, col = &quot;black&quot;, lwd = 2, lty = 2) We now turn to process \\(x_t\\). ## Simulate process x_t # initialize object to store result nsim &lt;- 10000 x_list &lt;- matrix(rep(NA, nsim*t_max), nrow = t_max, ncol = nsim) # simulation for (sim in 1:nsim) { set.seed(sim+123) z &lt;- rnorm(t_max+1) x_list[,sim] &lt;- z[2:(t_max+1)] + z[1:t_max] } All the simulated time series have mean zero in this case. As a consequence, we can use the sample mean \\(\\bar{x}_T\\) obtained from a single time series to estimate the (unconditional) mean of the process \\(\\mathbb{E}[x_t]\\). # plot plot.ts(x_list[,1], ylim = c(min(x_list), max(x_list)), main = &quot;Realizations of ergodic series&quot;, ylab = &quot;x&quot;) lines(x_list[,2], col = &quot;steelblue&quot;) lines(x_list[,3], col = &quot;tomato&quot;) abline(h = 0, col = &quot;black&quot;, lwd = 2, lty = 2) 1.2 Asymptotic results The problem with the series \\(\\{y_t\\}\\) in the previous example is that the autocovariance function does not decay as the lag order increases. Loosely speaking, the series gets stuck in the trajectory given by the initial draw of \\(U_0\\) and does not revert to the true mean of the stochastic process. The main consequence is that we cannot learn the mean of the process by taking the average of the observations in a single realized time series. It is true in general that some conditions on the rate of decay of the autocovariance are required to recover the mean of the process. For example, the Law of Large Numbers (LLN) guarantees that the sample mean converges in probability to the true mean under the assumption that the autocovariances are absolutely summable: \\[ \\sum_{k=0}^\\infty |\\gamma_k| &lt; \\infty \\] Notice that in the case of \\(\\{y_t\\}\\) above instead \\(\\sum_{k=0}^\\infty |\\gamma_k| = \\infty\\). On the contrary, \\(\\{x_t\\}\\) satisfies both the conditions of the LLN and of the Central Limit Theorem (CLT). The condition for the latter is that \\(\\{\\phi_k\\}_{k=0}^\\infty\\) is absolutely summable in \\(x_t = \\mu_x + \\sum_{k=0}^\\infty\\phi_k z_{t-k} = z_t + z_{t-1}\\), which is trivially verified. Therefore, since \\(\\mu_x = 0\\), \\[ \\sqrt{T} \\ \\bar{x}_T \\ \\xrightarrow{d} \\ \\mathcal{N}(0, \\sigma^2_{LR}), \\] with \\(\\sigma^2_{LR} = \\sum_{k=-\\infty}^\\infty \\gamma_k = \\text{Var}(x_t) + 2\\sum_{k=1}^\\infty \\gamma_k\\). ## Example 3: central limit theorem x_empirical &lt;- x_list[,9] x_means &lt;- colMeans(x_list) x_theory_variance &lt;- 4 # long run variance x_empir_variance &lt;- var(x_empirical) + 2*(cov(x_empirical[1:(t_max-1)], x_empirical[2:t_max])) rbind(empirical_variance = x_empir_variance, simulated_variance = var(x_means*sqrt(t_max)), theoretical_variance = x_theory_variance) ## [,1] ## empirical_variance 4.174614 ## simulated_variance 4.050667 ## theoretical_variance 4.000000 ## Plot empirical distribution VS. theoretical distribution hist(x_means*sqrt(t_max), breaks = 20, freq = FALSE, main = &quot;Distribution of the simulated means&quot;, xlab = expression(Simulated ~ means ~ rescaled ~ by ~ sqrt(T))) lines(density(x_means*sqrt(t_max)), lwd = 4, col = &quot;tomato&quot;) lines(density(rnorm(1e6, mean = 0, sd = sqrt(x_theory_variance))), lwd = 4, col = &quot;darkblue&quot;) 1.3 Empirical moments and summary statistics For this example we use the U.S. GDP data. x &lt;- read.csv(&quot;../data/us-gdp.csv&quot;)[,2] x &lt;- ts(x, start = c(1947, 1), frequency = 4) t_max &lt;- length(x) plot.ts(x, main = &quot;U.S. GDP&quot;, ylab = &quot;Billions of dollars&quot;) Consider the annualized quarterly growth rates: # xgrowth &lt;- ( (x[2:t_max]/x[1:(t_max-1)])^4 - 1 )*100 xgrowth &lt;- 4*diff(log(x))*100 xgrowth &lt;- ts(xgrowth, start = c(1947, 2), frequency = 4) plot.ts(xgrowth, main = &quot;Annualized GDP growth&quot;) Below we compute some summary statistics for the time series of the U.S. GDP growth rates. ## Example 4: empirical moments and summary statistics of the GDP growth library(moments) rbind( mean = mean(xgrowth), variance = var(xgrowth), skewness = skewness(xgrowth), kurtosis = kurtosis(xgrowth), min = min(xgrowth), max = max(xgrowth), above_5 = mean(xgrowth &gt; 5), annualized_volatility = sqrt(4)*sd(xgrowth) ) ## [,1] ## mean 6.1858847 ## variance 26.5117326 ## skewness -0.9793909 ## kurtosis 17.9597257 ## min -34.4929551 ## max 33.4065902 ## above_5 0.6078431 ## annualized_volatility 10.2979090 ## Autocovariance function gamma &lt;- function(x, k) { k &lt;- abs(k) t_max &lt;- length(x) # (t_max-k)/t_max * cov(x[1:(length(x)-k)], x[(k+1):length(x)]) # for compatibility with acf() cov(x[1:(t_max-k)], x[(k+1):t_max]) } ## Autocorrelation rho &lt;- function(x, k) {gamma(x, k) / gamma(x, 0)} ## autocorrelation at different lags sapply(0:12, rho, x = xgrowth) ## [1] 1.000000000 0.262228567 0.254514380 0.097922439 0.030475252 ## [6] -0.016390183 -0.003186283 0.054980002 0.062358818 0.146571003 ## [11] 0.169620140 0.143289793 0.096325249 Under the null hypothesis \\(H_0: \\rho = 0\\), the sample autocorrelation is distributed as \\[ \\sqrt{T} \\ \\hat{\\rho} \\xrightarrow{d} \\mathcal{N}(0, 1) \\] This means that the asymptotic variance of the estimator under the null is \\(1/T\\). The plot reports the 95% confidence interval obtained as \\(\\left(0 \\pm \\frac{z_{0.975}}{\\sqrt{T}}\\right)\\). ## confidence interval qnorm(0.975)*(1/sqrt(t_max)) ## [1] 0.1118611 ## using built-in function for the autocorrelogram acf(xgrowth, lag.max = 16) ## partial autocorrelation function pacf(xgrowth, lag.max = 16) 1.4 Hypothesis testing In this section we use three testing procedures on the GDP growth data: the Augmented Dickey-Fuller test for stationarity, the Jarque-Bera test for normality and the t-test for the mean of a process. ## Example 5: tests on GDP growth rates library(tseries) ## stationarity: augmented Dickey-Fuller adf.test(x) ## ## Augmented Dickey-Fuller Test ## ## data: x ## Dickey-Fuller = 2.4611, Lag order = 6, p-value = 0.99 ## alternative hypothesis: stationary adf.test(xgrowth) ## ## Augmented Dickey-Fuller Test ## ## data: xgrowth ## Dickey-Fuller = -5.9064, Lag order = 6, p-value = 0.01 ## alternative hypothesis: stationary ## normality: Jarque-Bera jarque.bera.test(xgrowth) ## ## Jarque Bera Test ## ## data: xgrowth ## X-squared = 2902.3, df = 2, p-value &lt; 2.2e-16 ## mean zero: t-test sigmaLR &lt;- sum(sapply(-100:100, gamma, x = xgrowth)) t_stat &lt;- mean(xgrowth)/(sqrt(sigmaLR/t_max)) p_value &lt;- (1-pnorm(abs(t_stat)))*2 # plot(density(rnorm(1e6)), xlim = c(-8, 8)) # abline(v = t_stat, col = &quot;tomato&quot;, lwd = 2) # compare with unadjusted variance # t.test(xgrowth, mu = 0) # mean(xgrowth)/(sqrt((t_max/(t_max-1))*var(xgrowth)/t_max)) # for compatibility with t-test() t_stat_unadjasted &lt;- mean(xgrowth)/(sqrt(var(xgrowth)/t_max)) p_value_unadjasted &lt;- (1-pnorm(abs(t_stat_unadjasted)))*2 rbind(p_value_unadjasted = p_value_unadjasted, p_value = p_value) ## [,1] ## p_value_unadjasted 0.000000e+00 ## p_value 4.884981e-15 # We can study when does the adjustment really matters # using the results of the previous Monte Carlo simulation ## Get variance and long-run variance sigma &lt;- apply(x_list, 2, function(x) sqrt(gamma(x, k = 0)/length(x))) sigmaLR &lt;- apply(x_list, 2, function(x) sqrt(sum(sapply(-3:3, gamma, x = x))/length(x))) ## Get adjusted and unadjusted t-statistics t_stat_unadjasted &lt;- x_means/sigma t_stat &lt;- x_means/sigmaLR ## Compute percentage of type-1 errors in the simulation type1error &lt;- mean(abs(t_stat) &gt; qnorm(0.975)) type1error_unadjusted &lt;- mean(abs(t_stat_unadjasted) &gt; qnorm(0.975)) rbind( type1error = type1error, type1error_unadjusted = type1error_unadjusted ) ## [,1] ## type1error 0.0569 ## type1error_unadjusted 0.1716 ## plot distributions of adjusted and unadjusted t-statistics hist(t_stat_unadjasted, breaks = 100, freq = FALSE, col = &quot;tomato&quot;, ylim = c(0, 0.45), xlim = c(-5, 6), xlab = &quot;t-stat&quot;, main = &quot;Distribution of the t-statistic in Monte Carlo simulation&quot;) hist(t_stat, breaks = 100, freq = FALSE, add = TRUE, col = &quot;lightgreen&quot;) lines(density(rnorm(1e6, mean = 0, sd = 1)), lwd = 4, col = &quot;black&quot;) legend(&quot;topright&quot;, c(&quot;Unadjusted&quot;, &quot;Adjusted&quot;), col=c(&quot;tomato&quot;, &quot;lightgreen&quot;), lwd=6) abline(v = qnorm(c(0.025, 0.975)), lty = 2, lwd = 2) text(x=c(6.5, 6.5), y=c(0.3, 0.25), labels=c(paste0(&quot;Type 1 error: &quot;, round(type1error_unadjusted*100, 1), &quot;%&quot;), paste0(&quot;Type 1 error adjusted: &quot;, round(type1error*100, 1), &quot;%&quot;)), col=c(&quot;tomato&quot;, &quot;lightgreen&quot;), pos = 2) "],["session02.html", "Session 2 Forecasting with time-series regression 2.1 Oracle inequality 2.2 Application: forecasting unemployment", " Session 2 Forecasting with time-series regression This session covers a first introduction to forecasting the conditional mean of a time series. A forecasting procedure involves three main objects: A set of predictors and a target variable that we want to forecast A class of forecasting models that we consider A loss function to evaluate the accuracy of the forecasts In this session we focus on the class of linear regression models estimated via least-squares. The loss function that we consider throughout this chapter is the quadratic loss function estimated using the Mean Squared Error (MSE) defined in the R function below. ## Function to compute the MSE. y = true value, f = forecast mse &lt;- function(y, f) {mean((y-f)^2)} The first section presents a simulation exercise to show the finite-sample properties of the linear regression model under mis-specification. The second section is an empirical application to the problem of forecasting the U.S. unemployment rate using some relevant macroeconomic indicators. 2.1 Oracle inequality Optional section, not covered in class Given a forecasting problem, the optimal prediction rule is defined as the one that minimizes the theoretical risk: \\[ f_{t+h|t}^* = \\arg\\min_{f \\in \\mathcal{F}} R(f_{t+h|t}) \\] and the associated optimal risk is denoted by \\(R^*\\). Under the square loss function, the optimal rule is the conditional expectation: \\[ f_{t+h|t}^* = \\mathbb{E}[y_{t+h}| x_t] \\] Notice that the exact functional form of the predictor is unknown: the conditional expected value could be of any linear or non-linear form. To make the theory of learning operational, we then have to restrict our attention to a class of prediction rules and pick the best rule from that class according to the limited amount of data available. A commonly used class of predictors is that of the linear predictors. Consider a linear regression model for predicting the target variable \\(y_{t+h}\\): \\[ f_{t+h|t} = x_t&#39;\\theta. \\] where \\(f_{t+h|t}\\) denotes the \\(h\\)-periods-ahead forecast. Irrespective of what is the true d.g.p., \\(\\theta\\) is the parameter that leads to the best forecasts within the class of linear regression models \\[ \\theta = \\arg\\min_{\\tilde{\\theta} \\in \\mathbb{R}^p} \\mathbb{E}[L(y_{t+h}, f_{t+h|t}(\\theta))] = R(\\theta) \\] However, in practice we can only use a finite sample to learn the parameters of the linear model via empirical risk minimization: \\[ \\hat{\\theta} = \\arg\\min_{\\theta \\in \\mathbb{R}^p} R_T(\\theta) \\] In our case, \\(R_T(\\theta) = \\frac{1}{T-h-t_w+1}\\sum_{t=t_w}^{T-h} (y_{t+h} - f_{t+h|t})^2 = MSE\\), where \\(t_w\\) denotes the initial number of observations used to train the model. So far, we have introduced three different measures of risk: \\(R^*\\), the risk associated with the theoretical optimal predictor if the true d.g.p. were known \\(R(\\theta)\\), the risk associated with the optimal predictor within the parametric class considered for learning \\(R_T(\\hat\\theta)\\), the empirical risk obtained by minimization of the loss function using the available data sample The regret (i.e. the difference between the optimal and the empirical risk) can be expressed in a way that links these three quantities: \\[ R_T(\\hat\\theta) - R^* = \\underbrace{\\left[ R(\\theta) - R^* \\right]}_{_{\\text{Approximation error}}} + \\underbrace{\\left[R_T(\\hat\\theta) - R(\\theta)\\right]}_{\\text{Estimation error}} \\] We mainly focus on the estimation error. In particular, we are usually interested in establishing finite-sample, probabilistic bounds to this quantity. These bounds are helpful in assessing, for instance, how many observations a model needs to perform well and how its performance relates to the number of predictors. For linear prediction, the following oracle inequality holds: there exists a constant \\(c\\) such that \\[ R(\\hat\\theta) \\leq R(\\theta) + c \\frac{p}{T}, \\quad \\text{w.p. } 1-1/T \\] The example below shows that the inequality holds in a simulation setting. We generate \\(10^6\\) observations from the following process: \\[ y_t = 0.6\\times y_{t-1} - \\tanh(0.3 \\times y_{t-1}) + 0.8\\times x_{1,t-1} + 0.2 \\times x_{2,t-1} - 0.7\\times x_{3,t-1} + \\varepsilon_t, \\] where \\(\\varepsilon_t \\sim \\mathcal{N}(0, 0.5)\\) and the variables \\(x_{1,t}, x_{2,t}, x_{3,t}\\) are drawn independently from a standard normal distribution. We then consider the class of linear regression models, \\[ y_t = \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 x_{1,t-1} + \\beta_3 x_{2,t-1} + \\beta_3 x_{3,t-1} + u_t \\] Notice that the OLS estimator is the empirical risk minimizer in the case of a square loss. In the simulation setting, we can also obtain \\(R(\\theta)\\) by estimating the parameters of the regression using the entire simulated population. We can then draw many estimates of \\(R(\\hat\\theta)\\) by considering different subsamples of the simulated population and estimating the parameters of the model using OLS on the subsample. By doing so, we can simulate the distribution of the empirical risk and compare it against the overall minimum risk within the class of linear estimators to assess the oracle inequality. We repeat the exercise considering different sizes of the subsamples to show that the bound gets tighter as the sample size increases, as it must be for the inequality to have the oracle property. ## Example 1: simulation for oracle inequality # build the simulated population dataset N &lt;- 3 T_pop &lt;- 1e5 set.seed(455) x_pop &lt;- matrix(rnorm((T_pop+1)*N), ncol = N) y_pop &lt;- rep(0, T_pop) for (t in 2:(T_pop+1)) { set.seed(456*t) # non-linear d.g.p. y_pop[t] &lt;- 0.6*y_pop[t-1] - tanh(0.3*y_pop[t-1]) + 0.8*x_pop[t-1, 1] + 0.2*x_pop[t-1, 2] - 0.7*x_pop[t-1, 3] + rnorm(1, sd = 0.5) } colnames(x_pop) &lt;- paste0(&quot;x&quot;, 1:N) # target is y(t+1). predictor is x(t) d_pop &lt;- as.data.frame(cbind(y = y_pop[2:(T_pop+1)] , x_pop[1:T_pop,] )) # estimate the linear predictor y(t) = x(t-1)&#39;b via OLS model_pop &lt;- lm(y ~ x1 + x2 + x3, data = d_pop) summary(model_pop) ## estimate the optimal risk and the population risk within the linear family # optimal f_optimal &lt;- rep(NA, T_pop+1) for (t in 1:T_pop) { f_optimal[t+1] &lt;- 0.6*y_pop[t] - tanh(0.3*y_pop[t]) + 0.8*x_pop[t, 1] + 0.2*x_pop[t, 2] - 0.7*x_pop[t, 3] } f_optimal &lt;- f_optimal[-1] risk_star &lt;- mse(d_pop[,1], f_optimal) # linear family risk_pop &lt;- mse(d_pop[,1], predict(model_pop)) ## Run the simulations to estimate a small-sample model many times t_small &lt;- 30 t_medium &lt;- 300 t_large &lt;- 3000 nsim &lt;- 1000 empirical_risk &lt;- list( model_small = rep(NA, nsim), model_medium = rep(NA, nsim), model_large = rep(NA, nsim) ) for (s in 1:nsim) { set.seed(456*s) # pick index at random between 1e5 and 9e5 idx &lt;- sample(1:(T_pop-t_large), 1) # consider the t subsequent observations d_small &lt;- d_pop[idx:(idx+t_small), ] d_medium &lt;- d_pop[idx:(idx+t_medium),] d_large &lt;- d_pop[idx:(idx+t_large), ] # estimate the model model_small &lt;- lm(y ~ x1 + x2 + x3, data = d_small) model_medium &lt;- lm(y ~ x1 + x2 + x3, data = d_medium) model_large &lt;- lm(y ~ x1 + x2 + x3, data = d_large) # compute the MSE and store the results empirical_risk$model_small[s] &lt;- mse(d_small[,1], predict(model_small)) empirical_risk$model_medium[s] &lt;- mse(d_medium[,1], predict(model_medium)) empirical_risk$model_large[s] &lt;- mse(d_large[,1], predict(model_large)) } # compute the constant C s.t. the bound risk_pop + C(P/t) is satisfied # 1-1/t fraction of times c_small &lt;- quantile(empirical_risk$model_small, 1-1/t_small) c_medium &lt;- quantile(empirical_risk$model_medium, 1-1/t_medium) c_large &lt;- quantile(empirical_risk$model_large, 1-1/t_large) ## report the distribution of the simulated empirical risks par(mfrow = c(2,2)) # small hist(empirical_risk$model_small, xlab = &quot;Empirical risk&quot;, main = &quot;Small model&quot;) abline(v = c_small, lwd = 2, lty = 2) abline(v = risk_pop, lwd = 3, lty = 1, col = &quot;darkred&quot;) text(x = c_small*1.2, y = 250, labels = paste0(&quot;Estimation error\\nbound: &quot;, round(c_small*N/t_small,3))) # medium hist(empirical_risk$model_medium, xlab = &quot;Empirical risk&quot;, main = &quot;Medium model&quot;) abline(v = c_medium, lwd = 2, lty = 2) abline(v = risk_pop, lwd = 3, lty = 1, col = &quot;darkred&quot;) text(x = c_medium*0.98, y = 150, labels = paste0(&quot;Estimation error\\nbound: &quot;, round(c_medium*N/t_medium,3))) # large hist(empirical_risk$model_large, xlab = &quot;Empirical risk&quot;, main = &quot;Large model&quot;) abline(v = c_large, lwd = 2, lty = 2) abline(v = risk_pop, lwd = 3, lty = 1, col = &quot;darkred&quot;) text(x = c_large*0.98, y = 120, labels = paste0(&quot;Estimation error\\nbound: &quot;, round(c_large*N/t_large,3))) 2.2 Application: forecasting unemployment library(sandwich) # for estimation of robust standard errors library(lmtest) # for coeftest library(forecast) # for dm.test We now consider an empirical application using the U.S. data from the FRED-MD dataset, a collection of 127 macroeconomic time series observed at a monthly frequency. To make sure that the series in the model are stationary, we transform the variables according to the recommendation of the authors, as summarised in the transformation code tcode in the first row of the dataset: tcode Transformation 1 No 2 \\(\\Delta x_t\\) 3 \\(\\Delta^2 x_t\\) 4 \\(\\log(x_t)\\) 5 \\(\\Delta\\log(x_t)\\) 6 \\(\\Delta^2\\log(x_t)\\) 7 \\(\\Delta\\left(\\frac{x_t}{x_{t-1}} -1 \\right)\\) ## load FRED-MD data fredmd &lt;- read.csv(&quot;../data/current.csv&quot;) dates &lt;- as.Date(fredmd$sasdate[-1],&#39;%m/%d/%Y&#39;) tcodes &lt;- fredmd[1,-1] d &lt;- fredmd[-1,-1] # subset 1980-2024 period data &lt;- d[dates &gt;= &#39;1980-01-01&#39;, ] dates &lt;- dates[dates &gt;= &#39;1980-01-01&#39;] t_max &lt;- length(dates) # choose variables target &lt;- &quot;UNRATE&quot; predictors &lt;- c(&#39;S.P.500&#39;,&#39;HOUST&#39;,&#39;UMCSENTx&#39;) vars &lt;- c(target, predictors) data &lt;- data[, vars] tcodes &lt;- tcodes[, vars] ## list of functions for transformation transform_fredmd &lt;- list( function(x) x, function(x) c(0, diff(x)), function(x) c(0, 0, diff(x, differences = 2)), function(x) log(x), function(x) c(0, diff(log(x))), function(x) c(0, 0, diff(log(x), differences = 2)), function(x) c(0, 0, diff( x[2:length(x)]/x[1:(length(x)-1)] - 1)) ) ## save the transformed series separately unrate &lt;- transform_fredmd[[tcodes$UNRATE]](data$UNRATE) sp500 &lt;- transform_fredmd[[tcodes$S.P.500]](data$S.P.500) house &lt;- transform_fredmd[[tcodes$HOUST]](data$HOUST) sent &lt;- transform_fredmd[[tcodes$UMCSENTx]](data$UMCSENTx) # plot par(mfrow = c(2,2)) plot.ts(ts(unrate, start = c(1980, 1), freq = 12), main = &quot;UNRATE&quot;, ylab = &quot;Month-on-month change&quot;) plot.ts(ts(sp500, start = c(1980, 1), freq = 12), main = &quot;S.P.500&quot;, ylab = &quot;Monthly % variation&quot;) plot.ts(ts(house, start = c(1980, 1), freq = 12), main = &quot;HOUST&quot;, ylab = &quot;log(housing starts)&quot;) plot.ts(ts(sent, start = c(1980, 1), freq = 12), main = &quot;UMCSENTx&quot;, ylab = &quot;Month-on-month change&quot;) We can have a look to the autocorrelation properties of the series. ## ACF plot par(mfrow = c(2,2)) acf(unrate, ylim=c(-0.1,1), lwd=5, xlim=c(0,25), main=&#39;UNRATE&#39;) acf(sp500, ylim=c(-0.1,1), lwd=5, xlim=c(0,25), main=&#39;S.P.500&#39;) acf(house, ylim=c(-0.1,1), lwd=5, xlim=c(0,25), main=&#39;HOUST&#39;) acf(sent, ylim=c(-0.1,1), lwd=5, xlim=c(0,25), main=&#39;UMCSENTx&#39;) We now study some in-sample properties of the predictive regression: \\[ \\begin{aligned} \\text{UNRATE}_t &amp;= \\beta_0 + \\beta_1 \\ \\text{UNRATE}_{t-1} + \\beta_2 \\ \\text{HOUST}_{t-1} \\ + \\beta_3 \\ \\text{SP500}_{t-1} \\ + \\beta_4 \\ \\text{SENT}_{t-1} \\ +\\\\ &amp;+ \\ \\text{dummies} \\ + \\varepsilon_t. \\end{aligned} \\] In particular, we compute the in-sample predictive fit of the model and we study the properties of the residuals. ## Fit predictive regression # Create the dataframe for regression data_regression &lt;- data.frame( unrate = unrate[2:t_max], unrate_lag = unrate[1:(t_max-1)], house = house[1:(t_max-1)], sp500 = sp500[1:(t_max-1)], sent = sent[1:(t_max-1)], d1 = (dates[2:t_max]==&quot;2020-04-01&quot;)*1, # covid dummy variable d2 = (dates[2:t_max]==&quot;2020-05-01&quot;)*1, d3 = (dates[2:t_max]==&quot;2020-06-01&quot;)*1, d4 = (dates[2:t_max]==&quot;2020-07-01&quot;)*1, d5 = (dates[2:t_max]==&quot;2020-08-01&quot;)*1, d6 = (dates[2:t_max]==&quot;2020-09-01&quot;)*1 ) # Fit OLS pred_model &lt;- lm(unrate ~ ., data = data_regression) vcov_nw &lt;- NeweyWest(pred_model, prewhite=F) # BAD inference (unadjusted asymptotic variance) coeftest(pred_model) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.6066299 0.1752768 3.4610 0.0005829 *** ## unrate_lag 0.1241838 0.0445711 2.7862 0.0055298 ** ## house -0.0854805 0.0244385 -3.4978 0.0005097 *** ## sp500 -0.4989494 0.2318030 -2.1525 0.0318242 * ## sent -0.0021749 0.0020826 -1.0443 0.2968167 ## d1 10.0608718 0.1859894 54.0938 &lt; 2.2e-16 *** ## d2 -2.8193347 0.4924891 -5.7247 1.762e-08 *** ## d3 -1.9965983 0.1882808 -10.6044 &lt; 2.2e-16 *** ## d4 -0.4795102 0.2010676 -2.3848 0.0174483 * ## d5 -1.6764097 0.1795464 -9.3369 &lt; 2.2e-16 *** ## d6 -0.2339512 0.1927566 -1.2137 0.2254137 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Inference using the Newey West estimator coeftest(pred_model, vcov_nw) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.6066299 0.3400340 1.7840 0.07501 . ## unrate_lag 0.1241838 0.0686069 1.8101 0.07087 . ## house -0.0854805 0.0464922 -1.8386 0.06655 . ## sp500 -0.4989494 0.2553605 -1.9539 0.05125 . ## sent -0.0021749 0.0024861 -0.8748 0.38207 ## d1 10.0608718 0.0970859 103.6286 &lt; 2.2e-16 *** ## d2 -2.8193347 0.7159484 -3.9379 9.351e-05 *** ## d3 -1.9965983 0.1022064 -19.5350 &lt; 2.2e-16 *** ## d4 -0.4795102 0.1528230 -3.1377 0.00180 ** ## d5 -1.6764097 0.0584554 -28.6785 &lt; 2.2e-16 *** ## d6 -0.2339512 0.1251049 -1.8700 0.06205 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # True VS. predicted values plot.ts(data_regression$unrate, ylim = c(-0.8, 0.8), col = &quot;darkgrey&quot;) lines(predict(pred_model), col = &quot;darkred&quot;) # Analysis of residuals acf(pred_model$residuals, lwd = 5) # Test for autocorrelation Box.test(pred_model$residuals, lag=12) ## ## Box-Pierce test ## ## data: pred_model$residuals ## X-squared = 76.202, df = 12, p-value = 2.176e-11 We can assess the effect of an exogenous change in the predictors on the target variable over time by using the method of local projections. Here we show the effect of a positive shock in the S.P.500 index. ## Local projections for assessing the impact of a shock # set number of horizons and initialize objects H &lt;- 12 lp &lt;- rep(0,H) # local projection lp_confint &lt;- matrix(0,H,2) # confidence intervals # compute the impact after h = 1,...,H horizons for(h in 1:H){ # prepare dataset for forecasting h periods ahead data_h &lt;- cbind( data_regression[h:t_max, c(&quot;unrate&quot;, paste0(&quot;d&quot;, 1:6))], data_regression[1:(t_max-h+1), c(&quot;unrate_lag&quot;, &quot;house&quot;, &quot;sp500&quot;, &quot;sent&quot;)] ) # predict pred_model_h &lt;- lm(unrate ~ ., data = data_h) # compute Newey-West estimate vcov_nw_h &lt;- NeweyWest(pred_model_h, prewhite = F) # compute standard errors for the estimated coefficients confint_h &lt;- coeftest(pred_model_h, vcov_nw_h) # store the estimates and the 90% confidence intervals lp[h] &lt;- confint_h[&#39;sp500&#39;,&#39;Estimate&#39;] lp_confint[h, ] &lt;- lp[h] + confint_h[&#39;sp500&#39;,&#39;Std. Error&#39;]*c(-1,1)*qnorm(0.05) } ## Plot the impulse response function plot(1:H, lp, lwd = 3, col = &#39;darkred&#39;, t = &quot;b&quot;, ylim = c(-2,1), main = &quot;Response of unemployment after a positive shock on the S.P.500&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;) lines(lp_confint[,1], lwd = 1, col = &#39;darkred&#39;, t = &quot;b&quot;, pch = 25) lines(lp_confint[,2], lwd = 1, col = &#39;darkred&#39;, t = &quot;b&quot;, pch = 24) grid() box() abline(h=0,lwd=3) We now proceed to evaluate the forecasts of the model. To do so, we split the data into a training set (observations up to 1999:12) and a testing set (starting in 2000:01). We estimate the parameters of the model on the training data and then make predictions based on the value of the predictors and the estimated coefficients. We compare the forecasts against the sample mean using the MSE. The Diebold-Mariano test suggests that the regression method has a superior forecasting performance than the sample mean. However, this result depends on the specification of the regression model and the predictive ability of the model is still pretty poor. ## Forecast evaluation # split training set (1980-2000) and testing set (2000-2024) in_sample &lt;- (dates[2:t_max] &lt; as.Date(&#39;2000-01-01&#39;)) out_sample &lt;- (dates[2:t_max] &gt;= as.Date(&#39;2000-01-01&#39;) &amp; ( dates[2:t_max] &lt; as.Date(&#39;2020-03-01&#39;) | dates[2:t_max] &gt;= as.Date(&#39;2020-09-01&#39;) )) data_train &lt;- data_regression[in_sample,] data_test &lt;- data_regression[out_sample,] dates_test &lt;- (dates[2:t_max])[out_sample] # fit predictive models model1 &lt;- lm(unrate ~ unrate_lag + sp500 + sent + house, data = data_train) model2 &lt;- lm(unrate ~ unrate_lag + sp500 + sent, data = data_train) model3 &lt;- lm(unrate ~ unrate_lag + sp500, data = data_train) # get predictions y_hat1 &lt;- predict(model1, newdata = data_test) y_hat2 &lt;- predict(model2, newdata = data_test) y_hat3 &lt;- predict(model3, newdata = data_test) # compute benchmark prediction (sample mean) y_benchmark &lt;- rep(mean(data_train$unrate), sum(out_sample)) # compute MSE mse_model1 &lt;- mse(data_test$unrate, y_hat1) mse_model2 &lt;- mse(data_test$unrate, y_hat2) mse_model3 &lt;- mse(data_test$unrate, y_hat3) mse_benchmark &lt;- mse(data_test$unrate, y_benchmark) # results round(rbind( mse_4vars = mse_model1, mse_3vars = mse_model2, mse_2vars = mse_model3, mse_mean = mse_benchmark, R2_2vars = 1 - mse_model3/mse_benchmark ), 4) ## [,1] ## mse_4vars 0.0536 ## mse_3vars 0.0302 ## mse_2vars 0.0296 ## mse_mean 0.0318 ## R2_2vars 0.0677 # convert to ts object to have dates in plot y_true &lt;- ts(data_test$unrate, start = c(2000, 1), frequency = 12) y_hat3 &lt;- ts(y_hat3, start = c(2000, 1), frequency = 12) y_benchmark &lt;- ts(y_benchmark, start = c(2000, 1), frequency = 12) # plot plot.ts(y_true, col = &quot;darkgrey&quot;, ylim = c(-0.4, 0.2), main = &quot;Forecasted values of unemployment change&quot;, ylab = &quot;Change in unemployment rate&quot;, xlab = &quot;&quot;) lines(y_hat3, col = &quot;darkred&quot;, lwd = 2) lines(y_benchmark) rect(2020.25,-1,2020.75,1,col = rgb(0.6,0.1,0.1,1/4)) legend(&quot;bottomleft&quot;, c(&quot;Forecast&quot;, &quot;True&quot;), col=c(&quot;darkred&quot;, &quot;darkgrey&quot;), lwd=6) It is common to report the plot of the normalized cumulated squared errors to assess if one model has a superior forecasting ability over all the testing period: \\[ \\text{Cumulated error}_t = \\frac{\\sum_{\\tau = t_w + 1}^t e_\\tau^2}{\\sum_{\\tau = t_w + 1}^T \\bar{e}_{\\tau}^2} \\] where \\(e_\\tau, \\bar{e}_\\tau\\) denote respectively the forecast error of the model and the benchmark at time \\(\\tau\\), and \\(t_w\\) is the number of periods in the training set. ## Plot the forecast errors over time # compute the forecast errors err_model &lt;- y_true-y_hat3 err_benchmark &lt;- y_true-y_benchmark # compute the (normalized) cumulated squared errors and convert to ts object err2_model &lt;- ts(cumsum((err_model)^2), start = c(2000, 1), frequency = 12) err2_benchmark &lt;- ts(cumsum((err_benchmark)^2), start = c(2000, 1), frequency = 12) err2_constant &lt;- sum((y_true-y_benchmark)^2) # plot the cumulated squared errors over time plot.ts(err2_benchmark/err2_constant, lwd = 2, ylab = &quot;Cumulated squared errors&quot;, main = &quot;Forecast errors over time&quot;, xlab = &quot;&quot;) lines(err2_model/err2_constant, col = &quot;darkred&quot;, lwd = 2) rect(2020.25,-1,2020.75,1.5,col = rgb(0.6,0.1,0.1,1/4)) legend(&quot;topleft&quot;, c(&quot;Regression model&quot;, &quot;Sample mean&quot;), col=c(&quot;darkred&quot;, &quot;black&quot;), lwd=6) ## Diebold-Mariano test for predictive performance # one-sided test: # - H0: err_benchmark = err_model # - H1: err_benchmark &gt; err_model dm.test(err_benchmark, err_model, alternative = &quot;greater&quot;) ## ## Diebold-Mariano Test ## ## data: err_benchmarkerr_model ## DM = 2.4995, Forecast horizon = 1, Loss function power = 2, p-value = ## 0.006504 ## alternative hypothesis: greater "],["session03.html", "Session 3 Large-dimensional methods for forecasting 3.1 Preprocess FRED-MD 3.2 Forecasting algorithms 3.3 Forecast evaluation", " Session 3 Large-dimensional methods for forecasting In this session we compare the performances of four forecasting methods to predict the monthly change in the U.S. unemployment rate using more than 100 variables from the FRED-MD dataset. In order to deal with the large amount of data, we consider methods based on principal component analysis, penalized likelihood and regression trees. 3.1 Preprocess FRED-MD The preprocessing of the data is very similar to that of the previous session. The only difference is that we now transform all the series in the dataset using the list of transformation functions of the previous session. For simplicity, here we remove the columns with missing values. An alternative could be to impute the missing values using some regression technique. However, it is important to notice that in the latter case we should impute the missing values at each step of the forecast evaluation to make sure that no future information is included in the forecasts. ## load data fredmd &lt;- read.csv(&quot;../data/current.csv&quot;) dates &lt;- as.Date(fredmd$sasdate[-1],&#39;%m/%d/%Y&#39;) target_variables &lt;- c(&quot;UNRATE&quot;, &quot;W875RX1&quot;, &quot;GS10&quot;, &quot;CPIAUCSL&quot;, &quot;WPSFD49207&quot;, &quot;PAYEMS&quot;, &quot;HOUST&quot;, &quot;INDPRO&quot;, &quot;M2SL&quot;, &quot;S.P.500&quot;) ## get tcodes tcodes &lt;- fredmd[1,-1] d &lt;- fredmd[-1, -1] ## Transform variables # transformation functions transform_fredmd &lt;- list( function(x) x, function(x) c(0, diff(x)), function(x) c(0, 0, diff(x, differences = 2)), function(x) log(x), function(x) c(0, diff(log(x))), function(x) c(0, 0, diff(log(x), differences = 2)), function(x) c(0, 0, diff( x[2:length(x)]/x[1:(length(x)-1)] - 1)) ) # transform series one by one for (j in 1:ncol(d)) { tcode &lt;- tcodes[[j]] d[,j] &lt;- transform_fredmd[[tcode]](d[,j]) } ## Subset and remove columns with NAs d &lt;- d[dates &gt;= &#39;1980-01-01&#39;, ] dates &lt;- dates[dates &gt;= &#39;1980-01-01&#39;] idx_na &lt;- (colSums(is.na(d))==0) d &lt;- d[, idx_na] plot.ts(d[,target_variables]) 3.2 Forecasting algorithms Below we define the forecasting algorithms that we use in the forecast evaluation. Notice that at each step of the forecast evaluation we rescale the data so that each variable has mean zero and unit variance. This step is very important for many machine learning algorithms. If the variables have a different magnitude, the optimization procedure involved tend to over-focus on the features with higher magnitude, while we want to exploit the variability of each feature irrespectively of its scale. Also, we do not explicitly account for outliers in the forecasting algorithms, and we simply discard the Covid period during the evaluation of the results. In this sense, the procedure will tend to benefit those methods that can automatically deal better with the presence of outliers in the data used for estimation. ## Principal Component Regression (PCR) pcr &lt;- function(d, target, n_components, horizon) { ## Scale the data d_scaled &lt;- scale(d) d_mean &lt;- attr(d_scaled, &quot;scaled:center&quot;) d_sd &lt;- attr(d_scaled, &quot;scaled:scale&quot;) ## get the target target_idx &lt;- which(colnames(d) == target) y &lt;- d_scaled[,target_idx] ## get the principal components of the other variables x &lt;- d_scaled[,-target_idx] eigen_list &lt;- prcomp(x, center=FALSE) f &lt;- eigen_list$x[,1:n_components] ## define the dataframes t_max &lt;- length(y) d_pcr &lt;- as.data.frame(cbind( y = y[(horizon+1):t_max], y_lag = y[1:(t_max-horizon)], f[1:(t_max-horizon),]) ) ## fit model &lt;- lm(y ~ ., data = d_pcr) ## forecast pred &lt;- predict(model, newdata = d_pcr[nrow(d_pcr), ]) ## output pred*d_sd[target] + d_mean[target] } # pcr(d, &quot;INDPRO&quot;, 4, 1) ## Penalized regression: LASSO (alpha = 1) and ridge (alpha = 0) library(glmnet) ## Caricamento del pacchetto richiesto: Matrix ## Loaded glmnet 4.1-4 penalized_reg &lt;- function(d, target, horizon, alpha = 1, nlambda = 100) { ## Scale the data d_scaled &lt;- scale(d) d_mean &lt;- attr(d_scaled, &quot;scaled:center&quot;) d_sd &lt;- attr(d_scaled, &quot;scaled:scale&quot;) ## get the target and predictors t_max &lt;- nrow(d) y &lt;- d_scaled[(horizon+1):t_max, target] x &lt;- d_scaled[1:(t_max-horizon), ] # include also lagged target variable ## fit the model model &lt;- glmnet(x = x, y = y, family = &quot;gaussian&quot;, alpha = alpha, nlambda = nlambda) ## forecast pred &lt;- predict(model, newx = x[nrow(x),]) ## Output pred*d_sd[target] + d_mean[target] } # penalized_reg(d, &quot;INDPRO&quot;, 1, 1) # one value for each lambda ## Random forest library(randomForest) forest &lt;- function(d, target, horizon, seed = 1, ntree = 100) { ## Scale the data d_scaled &lt;- scale(d) d_mean &lt;- attr(d_scaled, &quot;scaled:center&quot;) d_sd &lt;- attr(d_scaled, &quot;scaled:scale&quot;) ## get the target and predictors t_max &lt;- nrow(d) y &lt;- d_scaled[(horizon+1):t_max, target] x &lt;- d_scaled[1:(t_max-horizon), ] # include also lagged target variable ## fit the model set.seed(seed) model &lt;- randomForest(x = x, y = y, ntree = ntree) ## forecast pred &lt;- predict(model, newdata = x[nrow(x),]) ## Output pred*d_sd[target] + d_mean[target] } # forest(d, &quot;INDPRO&quot;, 1) 3.3 Forecast evaluation We can now run the algorithms and compare their forecasting performances over the 2002-2020 period. ## Initialize objects forecast_list &lt;- list( pcr = c(), ridge = list(), lasso = list(), forest = c() ) # choose target variable and forecast horizon target &lt;- &quot;UNRATE&quot; horizon &lt;- 1 # pick the initial window for training and choose the test period initial_window &lt;- sum(dates &lt; &quot;2002-01-01&quot;) idx_eval &lt;- which(((dates &lt; &quot;2020-02-01&quot;)) &amp; dates &gt;= &quot;2002-01-01&quot;) - initial_window # total steps of the forecast evaluation and selection of the test data n_steps &lt;- nrow(d) - initial_window - horizon + 1 data_test &lt;- d[(initial_window+horizon):nrow(d), target] ## expanding window forecast evaluation for (s in 1:n_steps) { cat(&quot;\\rStep&quot;, s, &quot;of&quot;, n_steps) data_train &lt;- d[1:(initial_window+s-1),] forecast_list$pcr[s] &lt;- pcr(data_train, target, n_components = 4, horizon = horizon) forecast_list$ridge[[s]] &lt;- penalized_reg(data_train, target, horizon = horizon, alpha = 0) forecast_list$lasso[[s]] &lt;- penalized_reg(data_train, target, horizon = horizon, alpha = 1) forecast_list$forest[s] &lt;- forest(data_train, target, horizon = horizon, ntree = 100) } Remark: here we just pick one value of \\(\\lambda\\) among the many that we used for estimating ridge and LASSO regressions. A more rigourous approach would consist in using either information criteria or cross validation to pick the optimal \\(\\lambda\\). We omit this part for simplicity of exposition. ## Comppute the RMSE of the predictions rmse &lt;- function(y, f) {sqrt(mean((y-f)^2))} pred_ridge &lt;- sapply(forecast_list$ridge, function(x) x[30]) pred_lasso &lt;- sapply(forecast_list$lasso, function(x) x[8]) pred_mean &lt;- rep(mean(d[1:initial_window, target]), n_steps) rbind( mean = rmse(data_test[idx_eval], pred_mean[idx_eval]), pcr = rmse(data_test[idx_eval], forecast_list$pcr[idx_eval]), forest = rmse(data_test[idx_eval], forecast_list$forest[idx_eval]), ridge = rmse(data_test[idx_eval], pred_ridge[idx_eval]), lasso = rmse(data_test[idx_eval], pred_lasso[idx_eval]) ) ## [,1] ## mean 0.1600012 ## pcr 0.1513172 ## forest 0.1676554 ## ridge 0.1475805 ## lasso 0.1416051 ## Plot the forecasted values ground_truth &lt;- ts(data_test[idx_eval], start = c(2002, 1), freq = 12) pred_benchmark &lt;- ts(pred_mean[idx_eval], start = c(2002, 1), freq = 12) pred_pcr &lt;- ts(forecast_list$pcr[idx_eval], start = c(2002, 1), freq = 12) pred_lasso &lt;- ts(pred_lasso[idx_eval], start = c(2002, 1), freq = 12) plot.ts(ground_truth, type = &quot;l&quot;, lwd = 2, main = &quot;Forecasts VS. ground truth&quot;, ylab = &quot;UNRATE change&quot;) lines(pred_benchmark, type = &quot;l&quot;, col = &quot;darkgrey&quot;) lines(pred_pcr, type = &quot;b&quot;, col = &quot;darkred&quot;, pch = 18) lines(pred_lasso, type = &quot;b&quot;, col = &quot;steelblue&quot;, pch = 19) legend(&quot;topright&quot;, c(&quot;PCR&quot;, &quot;LASSO&quot;), col = c(&quot;darkred&quot;, &quot;steelblue&quot;), pch = c(18, 19)) ## Diebold-Mariano test library(forecast) err_benchmark &lt;- data_test[idx_eval] - pred_mean[idx_eval] err_lasso &lt;- data_test[idx_eval] - pred_lasso[idx_eval] err_pcr &lt;- data_test[idx_eval] - forecast_list$pcr[idx_eval] # LASSO dm.test(err_benchmark, err_lasso, alternative = &quot;greater&quot;) ## ## Diebold-Mariano Test ## ## data: err_benchmarkerr_lasso ## DM = 3.9017, Forecast horizon = 1, Loss function power = 2, p-value = ## 6.381e-05 ## alternative hypothesis: greater # PCR dm.test(err_benchmark, err_pcr, alternative = &quot;greater&quot;) ## ## Diebold-Mariano Test ## ## data: err_benchmarkerr_pcr ## DM = 1.0484, Forecast horizon = 1, Loss function power = 2, p-value = ## 0.1478 ## alternative hypothesis: greater ## plot the cumulated squared errors over time # compute the (normalized) cumulated squared errors and convert to ts object err2_benchmark &lt;- ts(cumsum((err_benchmark)^2), start = c(2002, 1), frequency = 12) err2_pcr &lt;- ts(cumsum((err_pcr)^2), start = c(2002, 1), frequency = 12) err2_lasso &lt;- ts(cumsum((err_lasso)^2), start = c(2002, 1), frequency = 12) err2_constant &lt;- sum(err_benchmark^2) # plot plot.ts(err2_benchmark/err2_constant, lwd = 2, ylab = &quot;Cumulated squared errors&quot;, main = &quot;Forecast errors over time&quot;, xlab = &quot;&quot;) lines(err2_pcr/err2_constant, col = &quot;steelblue&quot;, lwd = 2) lines(err2_lasso/err2_constant, col = &quot;darkred&quot;, lwd = 2) rect(2008,-1,2009.5,1.5,col = rgb(0.6,0.1,0.1,1/4)) legend(&quot;topleft&quot;, c(&quot;Sample mean&quot;, &quot;PCR&quot;, &quot;LASSO&quot;), col=c(&quot;black&quot;, &quot;steelblue&quot;, &quot;darkred&quot;), lwd=6) "],["session04.html", "Session 4 ARMA(1,1) model 4.1 MLE for the ARMA(1,1) 4.2 Exercises", " Session 4 ARMA(1,1) model 4.1 MLE for the ARMA(1,1) In this session we consider the ARMA(1,1) model: \\[ y_t = c + \\phi y_{t-1} + \\theta \\varepsilon_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim WN(0, \\sigma^2_\\varepsilon) \\] Below we simulate an ARMA(1,1) model with Gaussian iid noise terms: \\(\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2_\\varepsilon)\\). ## Setup parameters for simulation t_max &lt;- 800 y0 &lt;- 0 const &lt;- 0 phi &lt;- 0.9 theta &lt;- -0.5 sigma_eps &lt;- 1 ## Simulation ARMA(1,1) eps &lt;- rnorm(t_max+1, sd = sigma_eps) y &lt;- c() y[1] &lt;- y0 for (t in 2:t_max) { y[t] &lt;- const + phi*y[t-1] + theta*eps[t-1] + eps[t] } plot.ts(y) ## Autocorrelograms acf(y, lwd = 3) pacf(y, lwd = 3) We now turn to estimate the model by maximum likelihood. We can then compare the estimates with the true values of the parameters that we used to simulate the data. Notice that the distribution of observation \\(y_t\\) given all the information set \\(I_{t-1}\\) available up to time \\(t-1\\) is \\[ y_t | I_{t-1} \\sim \\mathcal{N}(c + \\phi y_{t-1} + \\theta \\varepsilon_{t-1}, \\ \\sigma^2_\\varepsilon). \\] Let $ = (c, , , ^2_)$ denote the vector of parameters of the model. The conditional likelihood can be obtained as: \\[ L(\\eta; y_1, \\dots, y_T) = \\prod_{t=2}^T \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\varepsilon}}} \\exp\\left( -\\frac{1}{2\\sigma^2_\\varepsilon} (y_t - c - \\phi y_{t-1} - \\theta \\varepsilon_{t-1} )^2 \\right) \\] where the conditioning is on the first observation \\(y_1\\). Taking the logarithm and defining \\(\\hat \\varepsilon_t = y_t - c - \\phi y_{t-1} - \\theta \\varepsilon_{t-1}\\) we get \\[ \\ell(\\eta ; y_1, \\dots, y_T) = - \\frac{T-1}{2} \\log(2\\pi\\sigma^2_\\varepsilon) - \\frac{1}{2\\sigma^2_\\varepsilon} \\sum_{t=2}^T \\hat\\varepsilon_t^2 \\] The estimate of the innovations can be obtained using a filtering procedure: for a given value of the parameters \\(\\eta = (c, \\phi, \\theta, \\sigma^2_\\varepsilon)\\), we set \\(\\hat\\varepsilon_1\\) equal to the deviation from the unconditional mean of the first observation, \\(\\hat\\varepsilon_1 = y_1 - \\frac{c}{1-\\phi}\\), and then we compute \\(\\hat\\varepsilon_{t} = y_t - c - \\phi y_{t-1} + \\theta \\hat\\varepsilon_{t-1}\\) recursively for \\(t = 2, \\dots, T\\). Then we can use the estimated sequence of innovations \\(\\{\\hat\\varepsilon_t\\}_{t=2}^T\\) to evaluate the log-likelihood in \\(\\eta\\). ## Estimation: filtering ## 1-step ahead conditional mean of ARMA(1,1) model arma_filter &lt;- function(x, params){ # get parameters const &lt;- params[1] phi &lt;- params[2] theta &lt;- params[3] ## initialize intercept, innovation mu &lt;- rep(0, length(x)) mu[1] &lt;- const/(1-phi) eps &lt;- rep(0, length(x)) eps[1] &lt;- x[1] - mu[1] # filter and innovations for(t in 2:length(x)){ mu[t] &lt;- const + phi*x[t-1] + theta*eps[t-1] eps[t] &lt;- x[t] - mu[t] } # output: estimated innovations eps } arma_loglik &lt;- function(x, params){ # params now include (c, phi, theta, sigma2) t_max &lt;- length(x) sigma2 &lt;- params[4] eps &lt;- arma_filter(x, params[1:3]) # compute loglik loglik &lt;- -0.5*t_max*log(2*pi*sigma2) - (1/(2*sigma2))*sum(eps^2) # nlm() minimizes functions --&gt; we swap the sign of the log-likelihood # (max L = min -L) -loglik } ## estimation param_estim &lt;- nlm(arma_loglik, p = c(0, 0.1, 0, 1), x = y)$estimate ## Warning in nlm(arma_loglik, p = c(0, 0.1, 0, 1), x = y): NA/Inf sostituito da ## valore massimo positivo ## Warning in nlm(arma_loglik, p = c(0, 0.1, 0, 1), x = y): NA/Inf sostituito da ## valore massimo positivo ## Warning in nlm(arma_loglik, p = c(0, 0.1, 0, 1), x = y): NA/Inf sostituito da ## valore massimo positivo cbind(c(const, phi, theta, sigma_eps), round(param_estim, 3)) ## [,1] [,2] ## [1,] 0.0 -0.005 ## [2,] 0.9 0.873 ## [3,] -0.5 -0.515 ## [4,] 1.0 1.030 4.2 Exercises Exercise 1 (LASSO as OLS with thresholding) Consider the linear regression model \\(y = X\\beta + \\varepsilon\\) with uncorrelated regressors: \\(X&#39;X = I_n\\), where \\(X\\in\\mathbb{R}^{T\\times n}\\). Show that the LASSO estimator \\[ \\hat\\beta^{(\\lambda)} = \\arg\\min_{\\beta \\in \\mathbb{R}^n} \\ (y-X\\beta)&#39;(y-X\\beta) + \\lambda ||\\beta||_1 \\] can be written as \\[ \\hat\\beta_j^{(\\lambda)} = \\begin{cases} \\hat\\beta_j^{OLS} - \\frac{\\lambda}{2} &amp; \\qquad \\text{if} \\quad \\hat\\beta_j^{OLS} &gt; \\frac{\\lambda}{2} \\\\ \\hat\\beta_j^{OLS} + \\frac{\\lambda}{2} &amp; \\qquad \\text{if} \\quad \\hat\\beta_j^{OLS} &lt; -\\frac{\\lambda}{2} \\\\ 0 &amp; \\qquad \\text{if} \\quad \\hat\\beta_j^{OLS} \\in \\left[ -\\frac{\\lambda}{2}, \\frac{\\lambda}{2} \\right]\\\\ \\end{cases} \\] where \\(\\hat\\beta_j^{OLS}\\) denotes the \\(j\\)-th entry of the OLS estimator and \\(j = 1, \\dots, n\\). Exercise 2 (MA(\\(\\infty\\)) representation of stationary ARMA(1,1)) Show that the ARMA(1,1) process with \\(|\\phi| &lt; 1\\) \\[ y_t = c + \\phi y_{t-1} + \\theta \\varepsilon_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim WN(0, \\sigma^2_\\varepsilon) \\] can be rewritten as \\[ y_t = \\mu + \\sum_{j=0}^\\infty \\psi_j \\varepsilon_{t-j}, \\] for \\(\\mu = \\frac{c}{1-\\phi}\\), \\(\\psi_0 = 1\\), \\(\\psi_j = (\\phi + \\theta)\\phi^{j-1}\\). Exercise 3 (autocovariance function of the ARMA(1,1)) Show that the covariance function \\(\\gamma(k) = \\text{cov}(y_t, y_{t-k})\\) of the ARMA(1,1) process \\[ y_t = c + \\phi y_{t-1} + \\theta \\varepsilon_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim WN(0, \\sigma^2_\\varepsilon) \\] with \\(|\\phi &lt; 1|\\) can be written as \\[ \\gamma(k) = \\begin{cases} \\sigma^2_\\varepsilon \\left(1 + \\frac{(\\phi+\\theta)^2}{1-\\phi^2}\\right) &amp; \\qquad \\text{for} \\quad k=0 \\\\[1em] \\sigma^2_\\varepsilon \\left(\\phi+\\theta + \\frac{(\\phi+\\theta)^2\\phi}{1-\\phi^2}\\right) &amp; \\qquad \\text{for} \\quad k=1 \\\\[1em] \\phi^{k-1} \\ \\gamma(1) &amp; \\qquad \\text{for} \\quad k&gt;1 \\end{cases} \\] Hint: you may use the fact that for a linear time series \\(y_t = \\mu + \\sum_{j=0}^\\infty \\psi_j \\varepsilon_{t-j}\\) the autocovariance function is given by \\[ \\gamma(k) = \\sigma^2_\\varepsilon \\sum_{j=k}^\\infty \\psi_j \\psi_{j-k}, \\] provided that \\(\\sum_{j=0}^{\\infty} \\psi_j^2 &lt; \\infty\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
