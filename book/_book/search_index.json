[["index.html", "Financial Econometrics - Tutorials About", " Financial Econometrics - Tutorials Alessandro Ciancetta 2024-01-17 About TA materials for the Financial Econometrics course held by Prof.Â Christian Brownlees at the Barcelona School of Economics. "],["session01.html", "Session 1 Introduction to time series 1.1 Stochastic processes and dependence 1.2 Asymptotic results 1.3 Empirical moments and summary statistics 1.4 Hypothesis testing", " Session 1 Introduction to time series 1.1 Stochastic processes and dependence Stochastic processes allow for dependence in consecutive random variables \\(\\{\\dots, Y_{-2}, Y_{-1}, Y_{0}, Y_{1}, Y_{2} \\dots\\}\\). However, in practice, when we observe an empirical time series we are considering one, truncated realization of the stochastic process \\(\\{y_1, y_2, \\dots, y_T\\}\\). As it is easy to imagine, this can cause some issues in studying the properties of an empirical time series. First, because we can only study the finite dimensional distribution of the process. Second, because our task is to learn something about the process using a single realization. To overcome this limitations we need assumptions. In particular, two common assumptions in time series analysis are, loosely speaking: stationarity: the observed values in the sequence come from the same distribution, so that it is possible to learn from the past observations and to generalize the results to the entire, infinite stochastic process ergodicity: values observed far away in time can be considered as independent, so that the sufficiently long series can be considered as a representative sample of the underlying distribution Under these (or similar) assumptions, we can use the observations within a single observed time series to learn the parameters of our models. Stationarity ## Example 1: non-stationary series # simulation t_max &lt;- 500 y &lt;- rep(NA, t_max) for (t in 1:t_max) { if (t&lt;=100) { y[t] &lt;- rnorm(1, mean = 0, sd = 0.1) } if (t&gt;100 &amp; t&lt;=250) { y[t] &lt;- rnorm(1, mean = 0.5, sd = 0.1) } if (t&gt;250 &amp; t&lt;=400) { y[t] &lt;- rnorm(1, mean = 0, sd = 0.2) } if (t&gt;400 &amp; t&lt;=t_max) { y[t] &lt;- rnorm(1, mean = -0.2, sd = 0.05) } } # plot plot.ts(y, main = &quot;Non-stationary series&quot;) Ergodicity This examples shows how the assumptions of stationarity and ergodicity allow to learn the parameters of the distribution of the stochastic process from an empirical time series. Let \\(\\{z_t\\}_{t = 1}^{T} \\stackrel{iid}{\\sim} \\mathcal{N}(0, 1)\\) \\[ \\begin{aligned} y_t &amp;= U_0 + 0.25z_t, \\quad U_0 \\sim \\mathcal{N}(0, 10) \\\\ x_t &amp;= z_t + z_{t-1} \\end{aligned} \\] We have: \\[ \\mathbb{E}[y_t] = \\mathbb{E}[x_t] = 0 \\] \\[ \\begin{aligned} \\text{cov}(y_t, y_{t-1}) &amp;= \\begin{cases} 1+0.25^2 &amp; h = 0 \\\\ 1 &amp; h \\neq 0 \\end{cases} \\\\[1em] \\text{cov}(x_t, x_{t-1}) &amp;= \\begin{cases} 2 &amp; h = 0 \\\\ 1 &amp; h = 1 \\\\ 0 &amp; h \\geq 2 \\end{cases} \\end{aligned} \\] ## Example 2: weak dependence # initialize object to store result nsim &lt;- 3 y_list &lt;- matrix(rep(NA, nsim*t_max), nrow = t_max, ncol = nsim) # simulation for (sim in 1:nsim) { set.seed(sim+123) U0 &lt;- rnorm(1, sd = 10) z &lt;- rnorm(t_max) y_list[,sim] &lt;- U0 + z } # plot plot.ts(y_list[,1], ylim = c(min(y_list), max(y_list)), main = &quot;Realizations of non-ergodic series&quot;, ylab = &quot;y&quot;) lines(y_list[,2], col = &quot;steelblue&quot;) lines(y_list[,3], col = &quot;tomato&quot;) abline(h = 0, col = &quot;black&quot;, lwd = 2, lty = 2) # initialize object to store result nsim &lt;- 10000 x_list &lt;- matrix(rep(NA, nsim*t_max), nrow = t_max, ncol = nsim) # simulation for (sim in 1:nsim) { set.seed(sim+123) z &lt;- rnorm(t_max+1) x_list[,sim] &lt;- z[2:(t_max+1)] + z[1:t_max] } # plot plot.ts(x_list[,1], ylim = c(min(x_list), max(x_list)), main = &quot;Realizations of ergodic series&quot;, ylab = &quot;y&quot;) lines(x_list[,2], col = &quot;steelblue&quot;) lines(x_list[,3], col = &quot;tomato&quot;) abline(h = 0, col = &quot;black&quot;, lwd = 2, lty = 2) 1.2 Asymptotic results The problem with the series \\(\\{y_t\\}\\) in the previous example is that the autocovariance function does not decay. Loosely speaking, the series gets stuck in the trajectory given by the initial draw of \\(U_0\\) and does not revert to the true mean of the stochastic process. The main consequence is that we cannot learn the mean of the process by taking the average of the observations in a given time series. The fact that some conditions about the decay of the autocovariance are required to recover the mean of the process is true in general. For example, the Law of Large Numbers (LLN) guarantees that the sample mean converges in probability to the true mean under the assumption that the autocovariances are absolutely summable: \\[ \\sum_{k=0}^\\infty |\\gamma_k| &lt; \\infty \\] Notice that in the case of \\(\\{y_t\\}\\) above instead \\(\\sum_{k=0}^\\infty |\\gamma_k| = \\infty\\). On the contrary, \\(\\{x_t\\}\\) satisfies both the conditions of the LLN and of the Central Limit Theorem (CLT). The condition for the latter is that \\(\\{\\phi_k\\}_{k=0}^\\infty\\) is absolutely summable in \\(x_t = \\mu + \\sum_{k=0}^\\infty\\phi_k z_{t-k} = z_t + z_{t-1}\\), which is trivially verified. Therefore, since \\(\\mu_x = 0\\), \\[ \\sqrt{T} \\ \\bar{x}_T \\ \\xrightarrow{d} \\ \\mathcal{N}(0, \\sigma^2_{LR}), \\] with \\(\\sigma^2_{LR} = \\sum_{k=-\\infty}^\\infty \\gamma_k = \\text{Var}(x_t) + 2\\sum_{k=1}^\\infty \\gamma_k\\). ## Example 3: central limit theorem x_empirical &lt;- x_list[,9] x_means &lt;- colMeans(x_list) x_theory_variance &lt;- 4/t_max rbind(empirical_variance = var(x_empirical) + 2*(cov(x_empirical[1:(t_max-1)], x_empirical[2:t_max])), simulated_variance = var(x_means*sqrt(t_max)), theoretical_variance = x_theory_variance*t_max) ## [,1] ## empirical_variance 4.174614 ## simulated_variance 4.050667 ## theoretical_variance 4.000000 ## Plot empirical distribution VS. theoretical distribution hist(x_means, breaks = 20, freq = FALSE, main = &quot;Distribution of the simulated means&quot;, xlab = &quot;Simulated means&quot;) lines(density(x_means), lwd = 4, col = &quot;tomato&quot;) lines(density(rnorm(1e6, mean = 0, sd = sqrt(x_theory_variance))), lwd = 4, col = &quot;darkblue&quot;) 1.3 Empirical moments and summary statistics For this example we use the . x &lt;- read.csv(&quot;../us-gdp.csv&quot;)[,2] x &lt;- ts(x, start = c(1947, 1), frequency = 4) t_max &lt;- length(x) plot.ts(x, main = &quot;U.S. GDP&quot;, ylab = &quot;Billions of dollars&quot;) We consider the annualized quarterly growth rates: # xgrowth &lt;- ( (x[2:t_max]/x[1:(t_max-1)])^4 - 1 )*100 xgrowth &lt;- 4*diff(log(x))*100 xgrowth &lt;- ts(xgrowth, start = c(1947, 2), frequency = 4) plot.ts(xgrowth, main = &quot;Annualized GDP growth&quot;) ## Example 4: empirical moments and summary statistics of the GDP growth library(moments) rbind( mean = mean(xgrowth), variance = var(xgrowth), skewness = skewness(xgrowth), kurtosis = kurtosis(xgrowth), min = min(xgrowth), max = max(xgrowth), above_5 = mean(xgrowth &gt; 5), annualized_volatility = sqrt(4)*sd(xgrowth) ) ## [,1] ## mean 6.1858847 ## variance 26.5117326 ## skewness -0.9793909 ## kurtosis 17.9597257 ## min -34.4929551 ## max 33.4065902 ## above_5 0.6078431 ## annualized_volatility 10.2979090 ## Autocovariance function gamma &lt;- function(x, k) { k &lt;- abs(k) t_max &lt;- length(x) # (t_max-k)/t_max * cov(x[1:(length(x)-k)], x[(k+1):length(x)]) # for compatibility with acf() cov(x[1:(t_max-k)], x[(k+1):t_max]) } ## Autocorrelation rho &lt;- function(x, k) {gamma(x, k) / gamma(x, 0)} ## autocorrelation at different lags sapply(0:12, rho, x = xgrowth) ## [1] 1.000000000 0.262228567 0.254514380 0.097922439 0.030475252 ## [6] -0.016390183 -0.003186283 0.054980002 0.062358818 0.146571003 ## [11] 0.169620140 0.143289793 0.096325249 Under the null hypothesis \\(H_0: \\rho = 0\\), the sample autocorrelation is distributed as \\[ \\sqrt{T} \\ \\hat{\\rho} \\xrightarrow{d} \\mathcal{N}(0, 1) \\] This means that the asymptotic variance of the estimator under the null is \\(1/T\\). The plot reports the 95% confidence interval obtained as \\(\\left(0 \\pm \\frac{z_{0.975}}{\\sqrt{T}}\\right)\\). ## using built-in function for the autocorrelogram acf(xgrowth, lag.max = 16) ## partial autocorrelation function pacf(xgrowth, lag.max = 16) 1.4 Hypothesis testing In this section we use three testing procedures on the GDP growth data: the Augmented Dickey-Fuller test for stationarity, the Jarque-Bera test for normality and the t-test for the mean of a process. ## Example 5: tests on GDP growth rates library(tseries) ## stationarity: augmented Dickey-Fuller adf.test(x) ## ## Augmented Dickey-Fuller Test ## ## data: x ## Dickey-Fuller = 2.4611, Lag order = 6, p-value = 0.99 ## alternative hypothesis: stationary adf.test(xgrowth) ## ## Augmented Dickey-Fuller Test ## ## data: xgrowth ## Dickey-Fuller = -5.9064, Lag order = 6, p-value = 0.01 ## alternative hypothesis: stationary ## normality: Jarque-Bera jarque.bera.test(xgrowth) ## ## Jarque Bera Test ## ## data: xgrowth ## X-squared = 2902.3, df = 2, p-value &lt; 2.2e-16 ## mean zero: z-test sigmaLR &lt;- sum(sapply(-100:100, gamma, x = xgrowth)) t_stat &lt;- mean(xgrowth)/(sqrt(sigmaLR/t_max)) p_value &lt;- (1-pnorm(abs(t_stat)))*2 # t.test(xgrowth, mu = 0) # mean(xgrowth)/(sqrt((t_max/(t_max-1))*var(xgrowth)/t_max)) # for compatibility with t-test() t_stat_unadjasted &lt;- mean(xgrowth)/(sqrt(var(xgrowth)/t_max)) p_value_unadjasted &lt;- (1-pnorm(abs(t_stat_unadjasted)))*2 rbind(p_value_unadjasted = p_value_unadjasted, p_value = p_value) ## [,1] ## p_value_unadjasted 0.000000e+00 ## p_value 4.884981e-15 # We can study when adjustment really matters using the results of the previous # Monte Carlo simulation ybar &lt;- colMeans(x_list) sigma &lt;- apply(x_list, 2, function(x) sqrt(gamma(x, k = 0)/length(x))) sigmaLR &lt;- apply(x_list, 2, function(x) sqrt(sum(sapply(-3:3, gamma, x = x))/length(x))) t_stat &lt;- ybar/sigma t_stat_adjusted &lt;- ybar/sigmaLR type1error &lt;- mean(abs(t_stat) &gt; qnorm(0.975)) type1error_adj &lt;- mean(abs(t_stat_adjusted) &gt; qnorm(0.975)) # distributions hist(t_stat, breaks = 100, freq = FALSE, col = &quot;tomato&quot;, ylim = c(0, 0.45), xlim = c(-5, 6), xlab = &quot;t-stat&quot;, main = &quot;Distribution of the t-statistic in Monte Carlo simulation&quot;) hist(t_stat_adjusted, breaks = 100, freq = FALSE, add = TRUE, col = &quot;lightgreen&quot;) lines(density(rnorm(1e6, mean = 0, sd = 1)), lwd = 4, col = &quot;black&quot;) legend(&quot;topright&quot;, c(&quot;Unadjusted&quot;, &quot;Adjusted&quot;), col=c(&quot;tomato&quot;, &quot;lightgreen&quot;), lwd=6) abline(v = qnorm(c(0.025, 0.975)), lty = 2, lwd = 2) text(x=c(6.5, 6.5), y=c(0.3, 0.25), labels=c(paste0(&quot;Type 1 error: &quot;, round(type1error*100, 1), &quot;%&quot;), paste0(&quot;Type 1 error adjusted: &quot;, round(type1error_adj*100, 1), &quot;%&quot;)), col=c(&quot;tomato&quot;, &quot;lightgreen&quot;), pos = 2) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
